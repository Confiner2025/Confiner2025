<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training-free Long Video Generation with Chain of Diffusion Model Experts</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f9f9f9;
            color: #333;
        }
        header {
            text-align: center;
            margin-bottom: 40px;
        }
        h1 {
            font-family: 'Arial', sans-serif;
            font-weight: bold;
            font-size: 2.2em;
            color: #333;
            margin: 0;
            padding-bottom: 20px;
            line-height: 1.2;
        }
        .authors {
            font-family: 'Arial', sans-serif;
            font-size: 1.2em;
            color: #0066cc;
            margin-top: 10px;
            margin-bottom: 10px;
        }
        .institutions {
            font-size: 1em;
            color: #333;
            margin-bottom: 10px;
        }
        .links {
            margin-top: 20px;
        }
        .links a {
            text-decoration: none;
            color: white;
            background-color: #333;
            padding: 10px 15px;
            border-radius: 5px;
            margin-right: 10px;
            font-family: 'Arial', sans-serif;
        }
        .links a:hover {
            background-color: #555;
        }
    </style>
</head>
<body>
    <header>
        <h1>FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention</h1>
        <div class="authors">Wenhao Li<sup>1</sup>, Yichao Cao<sup>2</sup>, Xiu Su<sup>3</sup>, Xi Lin<sup>4</sup>, Shan You<sup>5</sup>, Mingkai Zheng<sup>1</sup>, Yi Chen<sup>6</sup>, Xu Chang<sup>1</sup></div>
        <div class="institutions"><sup>1</sup>University of Sydney, <sup>2</sup>Southeast University, <sup>3</sup>Central South University, <sup>4</sup>Shanghai Jiaotong University, <sup>5</sup>Sensetime Research, <sup>6</sup>Hong Kong University of Science and Technology</div>
        <div class="links">
            <a href="#">arXiv</a>
            <a href="#">Code (coming soon)</a>
        </div>
    </header>
    <section>
        <h2>Example Videos</h2>
        <div class="video-container">
            <video controls>
                <source src="panda.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video controls>
                <source src="city.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video controls>
                <source src="desert.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video controls>
                <source src="mountain.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video controls>
                <source src="wizard.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <video controls>
                <source src="penguin.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>
    <section>
        <h2>Pipeline</h2>
        <div class="results">
            <img src="main_fig-3.pdf" alt="Pipeline">
        </div>
    </section>
    <section>
        <h2>Abstract</h2>
        <p>
            Video generation models hold substantial potential in areas such as filmmaking. However, current video diffusion models need high computational costs and produce suboptimal results due to high complexity of video generation task. In this paper, we propose \textbf{ConFiner}, an efficient high-quality video generation framework that decouples video generation into easier subtasks: structure \textbf{con}trol and spatial-temporal re\textbf{fine}ment. It can generate high-quality videos with chain of off-the-shelf diffusion model experts, each expert responsible for a decoupled subtask. During the refinement, we introduce coordinated denoising, which can merge multiple diffusion experts' capabilities into a single sampling. Furthermore, we design ConFiner-Long framework, which can generate long coherent video with three constraint strategies on Confiner. Experimental results indicate that with only 10\% of the inference cost, our ConFiner surpasses representative models like Lavie and Modelscope across all objective and subjective metrics. And ConFiner-Long can generate high-quality and coherent videos with up to 600 frames.
    </section>
    <section>
        <h2>Experimental Results</h2>
        <div class="results">
            <img src="experiments.jpg" alt="Experimental Results">
        </div>
    </section>
</body>
</html>
